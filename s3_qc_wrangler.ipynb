{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import logging\n",
    "from botocore.exceptions import BotoCoreError, ClientError\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automate Production Center Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Set up configurations\n",
    "BUCKET_NAME = 'human-pangenomics'\n",
    "\n",
    "with open('config/dev/prefixes.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "    PREFIXES = config['PREFIXES']\n",
    "\n",
    "# Metadata files mapping\n",
    "metadata_files = {\n",
    "    'data/production-metadata/HPRC_RU_Y2_Sequel_Metadata_PacBio_HiFi_Submission.csv': 'RU_Y2_HIFI',\n",
    "    'data/production-metadata/HPRC_RU_Y2_topoff_Metadata_Submission.tsv': 'RU_Y2_topoff',\n",
    "    'data/production-metadata/HPRC_RU_Y3_HiFi_Metadata_Submission.csv': 'RU_Y3_HIFI',\n",
    "    'data/production-metadata/HPRC_RU_Y3_topoff_Metadata_Submission.tsv': 'RU_Y3_topoff',\n",
    "    'data/production-metadata/HPRC_RU_Y4_Metadata_Submission.tsv': 'RU_Y4'\n",
    "}\n",
    "\n",
    "INPUT_MAPPING_PATH = '/private/groups/hprc/human-pangenomics/hprc-synapse/config/dev/hifi_qc_input_mapping.csv'\n",
    "\n",
    "SRA_TABLE = pd.read_csv('/private/groups/hprc/human-pangenomics/hprc-synapse-1/HPRC_metadata/data/SRA/table_download.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 16:05:41,706 - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 16:05:42,054 - INFO - Processing project: RU_Y2_HIFI\n",
      "2024-11-05 16:05:42,056 - INFO - Created directory structure: submissions/RU_Y2_HIFI/hifi_qc\n",
      "/data/tmp/ipykernel_2418761/96568679.py:117: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  aws_df = aws_df[aws_df['Key'].str.contains(r'/(HG\\d+|NA\\d+)/', regex=True)]\n",
      "2024-11-05 16:05:42,443 - INFO - Data saved to submissions/RU_Y2_HIFI/RU_Y2_HIFI_aws_submissions_files.csv\n",
      "2024-11-05 16:05:42,446 - ERROR - Failed to copy template file to submissions/RU_Y2_HIFI/hifi_qc: [Errno 2] No such file or directory: '/private/groups/hprc/human-pangenomics/hprc-synapse/config/dev/hifi_qc_input_mapping.csv'\n",
      "2024-11-05 16:05:42,465 - INFO - QC script prepared for RU_Y2_HIFI at submissions/RU_Y2_HIFI/RU_Y2_HIFI_qc.py\n",
      "2024-11-05 16:05:42,467 - INFO - Processing project: RU_Y2_topoff\n",
      "2024-11-05 16:05:42,468 - INFO - Created directory structure: submissions/RU_Y2_topoff/hifi_qc\n",
      "/data/tmp/ipykernel_2418761/96568679.py:117: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  aws_df = aws_df[aws_df['Key'].str.contains(r'/(HG\\d+|NA\\d+)/', regex=True)]\n",
      "2024-11-05 16:05:42,539 - INFO - Data saved to submissions/RU_Y2_topoff/RU_Y2_topoff_aws_submissions_files.csv\n",
      "2024-11-05 16:05:42,541 - ERROR - Failed to copy template file to submissions/RU_Y2_topoff/hifi_qc: [Errno 2] No such file or directory: '/private/groups/hprc/human-pangenomics/hprc-synapse/config/dev/hifi_qc_input_mapping.csv'\n",
      "2024-11-05 16:05:42,558 - INFO - QC script prepared for RU_Y2_topoff at submissions/RU_Y2_topoff/RU_Y2_topoff_qc.py\n",
      "2024-11-05 16:05:42,559 - INFO - Processing project: RU_Y3_HIFI\n",
      "2024-11-05 16:05:42,560 - INFO - Created directory structure: submissions/RU_Y3_HIFI/hifi_qc\n",
      "/data/tmp/ipykernel_2418761/96568679.py:117: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  aws_df = aws_df[aws_df['Key'].str.contains(r'/(HG\\d+|NA\\d+)/', regex=True)]\n",
      "2024-11-05 16:05:42,737 - INFO - Data saved to submissions/RU_Y3_HIFI/RU_Y3_HIFI_aws_submissions_files.csv\n",
      "2024-11-05 16:05:42,759 - ERROR - Failed to copy template file to submissions/RU_Y3_HIFI/hifi_qc: [Errno 2] No such file or directory: '/private/groups/hprc/human-pangenomics/hprc-synapse/config/dev/hifi_qc_input_mapping.csv'\n",
      "2024-11-05 16:05:42,778 - INFO - QC script prepared for RU_Y3_HIFI at submissions/RU_Y3_HIFI/RU_Y3_HIFI_qc.py\n",
      "2024-11-05 16:05:42,780 - INFO - Processing project: RU_Y3_topoff\n",
      "2024-11-05 16:05:42,781 - INFO - Created directory structure: submissions/RU_Y3_topoff/hifi_qc\n",
      "/data/tmp/ipykernel_2418761/96568679.py:117: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  aws_df = aws_df[aws_df['Key'].str.contains(r'/(HG\\d+|NA\\d+)/', regex=True)]\n",
      "2024-11-05 16:05:42,996 - INFO - Data saved to submissions/RU_Y3_topoff/RU_Y3_topoff_aws_submissions_files.csv\n",
      "2024-11-05 16:05:43,000 - ERROR - Failed to copy template file to submissions/RU_Y3_topoff/hifi_qc: [Errno 2] No such file or directory: '/private/groups/hprc/human-pangenomics/hprc-synapse/config/dev/hifi_qc_input_mapping.csv'\n",
      "2024-11-05 16:05:43,018 - INFO - QC script prepared for RU_Y3_topoff at submissions/RU_Y3_topoff/RU_Y3_topoff_qc.py\n",
      "2024-11-05 16:05:43,019 - INFO - Processing project: RU_Y4\n",
      "2024-11-05 16:05:43,021 - INFO - Created directory structure: submissions/RU_Y4/hifi_qc\n",
      "/data/tmp/ipykernel_2418761/96568679.py:117: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  aws_df = aws_df[aws_df['Key'].str.contains(r'/(HG\\d+|NA\\d+)/', regex=True)]\n",
      "2024-11-05 16:05:43,163 - INFO - Data saved to submissions/RU_Y4/RU_Y4_aws_submissions_files.csv\n",
      "2024-11-05 16:05:43,166 - ERROR - Failed to copy template file to submissions/RU_Y4/hifi_qc: [Errno 2] No such file or directory: '/private/groups/hprc/human-pangenomics/hprc-synapse/config/dev/hifi_qc_input_mapping.csv'\n",
      "2024-11-05 16:05:43,185 - INFO - QC script prepared for RU_Y4 at submissions/RU_Y4/RU_Y4_qc.py\n",
      "2024-11-05 16:05:43,193 - INFO - Copied data/production-metadata/HPRC_RU_Y2_Sequel_Metadata_PacBio_HiFi_Submission.csv to submissions/RU_Y2_HIFI/HPRC_RU_Y2_Sequel_Metadata_PacBio_HiFi_Submission.csv\n",
      "2024-11-05 16:05:43,200 - INFO - Copied data/production-metadata/HPRC_RU_Y2_topoff_Metadata_Submission.tsv to submissions/RU_Y2_topoff/HPRC_RU_Y2_topoff_Metadata_Submission.tsv\n",
      "2024-11-05 16:05:43,206 - INFO - Copied data/production-metadata/HPRC_RU_Y3_HiFi_Metadata_Submission.csv to submissions/RU_Y3_HIFI/HPRC_RU_Y3_HiFi_Metadata_Submission.csv\n",
      "2024-11-05 16:05:43,213 - INFO - Copied data/production-metadata/HPRC_RU_Y3_topoff_Metadata_Submission.tsv to submissions/RU_Y3_topoff/HPRC_RU_Y3_topoff_Metadata_Submission.tsv\n",
      "2024-11-05 16:05:43,219 - INFO - Copied data/production-metadata/HPRC_RU_Y4_Metadata_Submission.tsv to submissions/RU_Y4/HPRC_RU_Y4_Metadata_Submission.tsv\n"
     ]
    }
   ],
   "source": [
    "# Initialize the S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def list_files_with_size(bucket, prefix):\n",
    "    \"\"\"List files in an S3 bucket with size details.\"\"\"\n",
    "    try:\n",
    "        file_list = []\n",
    "        paginator = s3.get_paginator('list_objects_v2')\n",
    "        pages = paginator.paginate(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "        for page in pages:\n",
    "            if 'Contents' in page:\n",
    "                for obj in page['Contents']:\n",
    "                    file_list.append({\n",
    "                        'Key': obj['Key'],\n",
    "                        'Size_in_Bytes': obj['Size'],\n",
    "                        'Size_in_GB': obj['Size'] / (1024 ** 3)  # Convert to GB\n",
    "                    })\n",
    "\n",
    "        return file_list\n",
    "    except (BotoCoreError, ClientError) as e:\n",
    "        logging.error(f\"Failed to list files in bucket {bucket} with prefix {prefix}: {e}\")\n",
    "        return []\n",
    "\n",
    "def create_directory_structure(project_name):\n",
    "    \"\"\"Create main and hifi_qc directories for each project under the submission folder.\"\"\"\n",
    "    main_dir = os.path.join('submissions', project_name)\n",
    "    hifi_qc_dir = os.path.join(main_dir, 'hifi_qc')\n",
    "    \n",
    "    os.makedirs(hifi_qc_dir, exist_ok=True)\n",
    "    logging.info(f\"Created directory structure: {hifi_qc_dir}\")\n",
    "    \n",
    "    return hifi_qc_dir\n",
    "\n",
    "def copy_metadata_files():\n",
    "    \"\"\"Copy each metadata file to its corresponding project folder within the submission directory.\"\"\"\n",
    "    for metadata_file, project_folder in metadata_files.items():\n",
    "        # Define the source path (metadata file already located in production-metadata directory)\n",
    "        source_path = metadata_file\n",
    "\n",
    "        # Define the destination path within the submission directory\n",
    "        destination_dir = os.path.join('submissions', project_folder)\n",
    "        destination_path = os.path.join(destination_dir, os.path.basename(metadata_file))\n",
    "\n",
    "        # Ensure the destination project directory exists\n",
    "        os.makedirs(destination_dir, exist_ok=True)\n",
    "        \n",
    "        # Copy the file\n",
    "        try:\n",
    "            shutil.copy(source_path, destination_path)\n",
    "            logging.info(f\"Copied {metadata_file} to {destination_path}\")\n",
    "        except IOError as e:\n",
    "            logging.error(f\"Failed to copy {metadata_file} to {destination_path}: {e}\")\n",
    "\n",
    "\n",
    "def save_aws_df_to_csv(aws_df, output_path):\n",
    "    \"\"\"Save the S3 file list DataFrame to CSV.\"\"\"\n",
    "    try:\n",
    "        aws_df.to_csv(output_path, index=False)\n",
    "        logging.info(f\"Data saved to {output_path}\")\n",
    "    except IOError as e:\n",
    "        logging.error(f\"Failed to save DataFrame to {output_path}: {e}\")\n",
    "\n",
    "def copy_template_file(dest_dir):\n",
    "    \"\"\"Copy the template file to the destination directory.\"\"\"\n",
    "    try:\n",
    "        shutil.copy(INPUT_MAPPING_PATH, os.path.join(dest_dir, \"hifi_qc_input_mapping.csv\"))\n",
    "        logging.info(f\"Template CSV copied to {dest_dir}\")\n",
    "    except IOError as e:\n",
    "        logging.error(f\"Failed to copy template file to {dest_dir}: {e}\")\n",
    "\n",
    "\n",
    "def prepare_qc_script(project_name):\n",
    "    \"\"\"Copy and customize the QC template script for each project.\"\"\"\n",
    "    # Define paths for the template and the destination script\n",
    "    template_script_path = 'config/dev/hifi_qc_template.py'\n",
    "    destination_script_path = os.path.join('submissions', project_name, f\"{project_name}_qc.py\")\n",
    "\n",
    "    # Copy the template script to the destination\n",
    "    try:\n",
    "        shutil.copy(template_script_path, destination_script_path)\n",
    "        \n",
    "        # Read in the script and replace placeholder with project name\n",
    "        with open(destination_script_path, 'r') as file:\n",
    "            script_content = file.read()\n",
    "        \n",
    "        # Replace \"PROJECT_NAME\" placeholder with the actual project name in the script content\n",
    "        script_content = script_content.replace(\"PROJECT_NAME\", project_name)\n",
    "        \n",
    "        # Write the customized script back to the file\n",
    "        with open(destination_script_path, 'w') as file:\n",
    "            file.write(script_content)\n",
    "        \n",
    "        logging.info(f\"QC script prepared for {project_name} at {destination_script_path}\")\n",
    "    except IOError as e:\n",
    "        logging.error(f\"Failed to prepare QC script for {project_name}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def process_prefix(project_name, prefix):\n",
    "    \"\"\"Process each prefix: fetch data from S3, save to CSV, and set up directories.\"\"\"\n",
    "    logging.info(f\"Processing project: {project_name}\")\n",
    "    \n",
    "    # Create necessary directories\n",
    "    hifi_qc_dir = create_directory_structure(project_name)\n",
    "    \n",
    "    # Get the list of files with their sizes for each prefix\n",
    "    aws_files_with_size = list_files_with_size(BUCKET_NAME, prefix)\n",
    "    if not aws_files_with_size:\n",
    "        logging.warning(f\"No files found for prefix {prefix}\")\n",
    "        return\n",
    "\n",
    "    # Convert the list to a DataFrame\n",
    "    aws_df = pd.DataFrame(aws_files_with_size)\n",
    "    aws_df['Key'] = 's3://' + BUCKET_NAME + '/' + aws_df['Key']\n",
    "    aws_df = aws_df[aws_df['Key'].str.contains(r'\\.bam$|\\.fastq\\.gz$', regex=True)]\n",
    "    aws_df = aws_df[aws_df['Key'].str.contains(r'/(HG\\d+|NA\\d+)/', regex=True)]\n",
    "    aws_df['Prefix_Label'] = project_name\n",
    "    \n",
    "    # Save the DataFrame to CSV under the main project directory\n",
    "    output_path = os.path.join('submissions', project_name, f\"{project_name}_aws_submissions_files.csv\")\n",
    "    save_aws_df_to_csv(aws_df, output_path)\n",
    "\n",
    "    # Copy template file to hifi_qc directory\n",
    "    copy_template_file(hifi_qc_dir)\n",
    "\n",
    "# Run the process for each prefix\n",
    "for project_name, prefix in PREFIXES.items():\n",
    "    process_prefix(project_name, prefix)\n",
    "    # Prepare the QC script for the project\n",
    "    prepare_qc_script(project_name)\n",
    "\n",
    "# Copy metadata files to the main project directories\n",
    "copy_metadata_files()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " automate output sample table check\n",
    "\n",
    "Required\n",
    "- Metadata_Submission\n",
    "- aws_submissions_files\n",
    "- aws_transfer_working\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Production Center Submission Files Released to Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 10:41:52,400 - INFO - All files in RU_Y2_HIFI.aws_transfer_working.csv are present in AWS.\n",
      "2024-11-06 10:41:52,680 - INFO - All files in RU_Y2_topoff.aws_transfer_working.csv are present in AWS.\n",
      "2024-11-06 10:41:56,596 - INFO - All files in RU_Y3_HIFI.aws_transfer_working.csv are present in AWS.\n",
      "2024-11-06 10:41:58,078 - INFO - All files in RU_Y3_topoff.aws_transfer_working.csv are present in AWS.\n",
      "2024-11-06 10:42:13,717 - INFO - All files in RU_Y4.aws_transfer_working.csv are present in AWS.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def check_s3_file_exists(s3_path):\n",
    "    \"\"\"Check if a specific file exists in the S3 bucket using its full S3 path.\"\"\"\n",
    "    # Parse the bucket and key from the S3 path\n",
    "    if not s3_path.startswith(\"s3://\"):\n",
    "        raise ValueError(\"Invalid S3 path format.\")\n",
    "    \n",
    "    path_parts = s3_path[5:].split('/', 1)\n",
    "    bucket_name = path_parts[0]\n",
    "    key = path_parts[1]\n",
    "\n",
    "    try:\n",
    "        # Attempt to retrieve metadata for the object to check existence\n",
    "        s3.head_object(Bucket=bucket_name, Key=key)\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        # If a 404 error is raised, the file does not exist\n",
    "        if e.response['Error']['Code'] == \"404\":\n",
    "            return False\n",
    "        else:\n",
    "            # Raise any other errors encountered\n",
    "            raise e\n",
    "\n",
    "def check_aws_transfer_files(project_name, prefix):\n",
    "    \"\"\"Check if all files in <project>.aws_transfer_working.csv are present in the S3 bucket.\"\"\"\n",
    "    transfer_csv_path = os.path.join('submissions', project_name, f\"{project_name}.aws_transfer_working.csv\")\n",
    "    \n",
    "    # Load the list of paths from the transfer CSV without any modification\n",
    "    try:\n",
    "        transfer_df = pd.read_csv(transfer_csv_path, header=None)\n",
    "        transfer_paths = transfer_df.iloc[:, 1].tolist()  # Direct list of paths from the CSV\n",
    "        \n",
    "        # Check each file in the transfer list and print only missing files\n",
    "        missing_files = [s3_path for s3_path in transfer_paths if not check_s3_file_exists(s3_path)]\n",
    "        \n",
    "        if missing_files:\n",
    "            logging.warning(f\"Missing files for project {project_name}:\")\n",
    "            for path in missing_files:\n",
    "                print(path)\n",
    "        else:\n",
    "            logging.info(f\"All files in {project_name}.aws_transfer_working.csv are present in AWS.\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"{transfer_csv_path} not found.\")\n",
    "\n",
    "# Run the check for each project and prefix\n",
    "for project_name, prefix in PREFIXES.items():\n",
    "    check_aws_transfer_files(project_name, prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check SRA Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values found in columns ['DeepConsensus_version', 'notes'] for submissions/RU_Y2_topoff/RU_Y2_topoff_data_table.csv\n",
      "NaN values found in columns ['DeepConsensus_version', 'notes'] for submissions/RU_Y3_HIFI/RU_Y3_HIFI_data_table.csv\n",
      "NaN values found in columns ['DeepConsensus_version', 'notes'] for submissions/RU_Y2_HIFI/RU_Y2_HIFI_data_table.csv\n",
      "NaN values found in columns ['DeepConsensus_version', 'notes'] for submissions/RU_Y3_topoff/RU_Y3_topoff_data_table.csv\n",
      "NaN values found in columns ['DeepConsensus_version', 'notes'] for submissions/RU_Y4/RU_Y4_data_table.csv\n",
      "All project directories contain a data table file.\n",
      "\n",
      "Files with NaN values in specific columns:\n",
      "submissions/RU_Y2_topoff/RU_Y2_topoff_data_table.csv has NaN values in columns: ['DeepConsensus_version', 'notes']\n",
      "submissions/RU_Y3_HIFI/RU_Y3_HIFI_data_table.csv has NaN values in columns: ['DeepConsensus_version', 'notes']\n",
      "submissions/RU_Y2_HIFI/RU_Y2_HIFI_data_table.csv has NaN values in columns: ['DeepConsensus_version', 'notes']\n",
      "submissions/RU_Y3_topoff/RU_Y3_topoff_data_table.csv has NaN values in columns: ['DeepConsensus_version', 'notes']\n",
      "submissions/RU_Y4/RU_Y4_data_table.csv has NaN values in columns: ['DeepConsensus_version', 'notes']\n",
      "\n",
      "Aggregated data saved to 'HPRC-aggregated_data_table.csv'\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the required columns\n",
    "required_columns = [\n",
    "    'filename', 'sample_ID', 'path', 'accession', 'study', 'biosample_accession',\n",
    "    'total_reads', 'total_bp', 'total_Gbp', 'min', 'max', 'mean', 'quartile_25',\n",
    "    'quartile_50', 'quartile_75', 'N25', 'N50', 'N75', 'library_ID',\n",
    "    'library_strategy', 'library_source', 'library_selection', 'library_layout',\n",
    "    'platform', 'instrument_model', 'design_description', 'data_type',\n",
    "    'shear_method', 'size_selection', 'DeepConsensus_version', 'polymerase_version',\n",
    "    'seq_plate_chemistry_version', 'generator_facility', 'generator_contact',\n",
    "    'notes', 'ccs_algorithm', 'MM_tag'\n",
    "]\n",
    "\n",
    "# Get list of all submission project directories\n",
    "submission_dirs = glob.glob('submissions/*/')\n",
    "# Get list of all data table files across submissions\n",
    "data_table_files = glob.glob('submissions/*/*_data_table.csv')\n",
    "\n",
    "# Extract project names that have data tables\n",
    "data_table_projects = {os.path.dirname(file) for file in data_table_files}\n",
    "\n",
    "# List to hold DataFrames, track missing columns, NaNs, and missing projects\n",
    "data_frames = []\n",
    "missing_data_table_projects = []\n",
    "missing_columns_files = []\n",
    "nan_columns_files = []\n",
    "\n",
    "# Loop through each data table file\n",
    "for file_path in data_table_files:\n",
    "    try:\n",
    "        # Read each file into a DataFrame\n",
    "        df = pd.read_csv(file_path)  # adjust 'sep' if necessary, e.g., ',' for CSV\n",
    "        data_frames.append(df)\n",
    "        \n",
    "        # Check for missing columns\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"Missing columns in {file_path}: {missing_columns}\")\n",
    "            missing_columns_files.append((file_path, missing_columns))\n",
    "        \n",
    "        # Check for NaN values in required columns\n",
    "        nan_columns = df[required_columns].isna().any()\n",
    "        nan_columns = nan_columns[nan_columns].index.tolist()\n",
    "        if nan_columns:\n",
    "            print(f\"NaN values found in columns {nan_columns} for {file_path}\")\n",
    "            nan_columns_files.append((file_path, nan_columns))\n",
    "        else:\n",
    "            print(f\"Loaded data from {file_path} with all required columns and no NaN values\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "\n",
    "# Identify projects missing a data table file\n",
    "for project_dir in submission_dirs:\n",
    "    if project_dir.rstrip('/') not in data_table_projects:\n",
    "        missing_data_table_projects.append(project_dir)\n",
    "\n",
    "# Print missing project directories\n",
    "if missing_data_table_projects:\n",
    "    print(\"\\nDirectories missing a data table file:\")\n",
    "    for missing_dir in missing_data_table_projects:\n",
    "        print(missing_dir)\n",
    "else:\n",
    "    print(\"All project directories contain a data table file.\")\n",
    "\n",
    "# Print files missing specific columns\n",
    "if missing_columns_files:\n",
    "    print(\"\\nFiles with missing columns:\")\n",
    "    for file_path, missing_columns in missing_columns_files:\n",
    "        print(f\"{file_path} missing columns: {missing_columns}\")\n",
    "\n",
    "# Print files with NaN values in specific columns\n",
    "if nan_columns_files:\n",
    "    print(\"\\nFiles with NaN values in specific columns:\")\n",
    "    for file_path, nan_columns in nan_columns_files:\n",
    "        print(f\"{file_path} has NaN values in columns: {nan_columns}\")\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame if they have the required columns\n",
    "if data_frames:\n",
    "    aggregated_df = pd.concat(data_frames, ignore_index=True)\n",
    "    # Save the aggregated DataFrame to a new CSV file\n",
    "    aggregated_df[required_columns].to_csv(\"data/hprc-data-explorer-tables/HPRC_PacBio_HiFi.file.index.csv\", index=False)\n",
    "    print(\"\\nAggregated data saved to 'HPRC-aggregated_data_table.csv'\")\n",
    "else:\n",
    "    print(\"No data tables found to aggregate.\")\n",
    "# SRA \n",
    "assert [filename for filename in aggregated_df.filename.tolist() if filename not in SRA_TABLE['SRA.filename'].tolist()] == []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation rules where filename and path align by index within each sample_ID\n",
    "aggregation_rules = {\n",
    "     'sample_ID': 'first', \n",
    "    'filename': lambda x: list(x),           # Collect filenames in a list, preserving order\n",
    "    'path': lambda x: list(x),               # Collect paths in a list, preserving order\n",
    "                      # Take the first sample_ID since it should be consistent per group\n",
    "    \n",
    "    # Additional fields with aggregation rules\n",
    "    'library_ID':  lambda x: list(x),\n",
    "    'library_strategy':  lambda x: list(x),\n",
    "    'library_source':  lambda x: list(x),\n",
    "    'library_selection':  lambda x: list(x),\n",
    "    'library_layout':  lambda x: list(x),\n",
    "    'platform':  lambda x: list(x),\n",
    "    'instrument_model':  lambda x: list(x),\n",
    "    'design_description':  lambda x: list(x),\n",
    "    'data_type':  lambda x: list(x),\n",
    "    'shear_method':  lambda x: list(x),\n",
    "    'size_selection':  lambda x: list(x),\n",
    "    'ccs_algorithm':  lambda x: list(x),\n",
    "    'polymerase_version':  lambda x: list(x),\n",
    "    'seq_plate_chemistry_version':  lambda x: list(x),\n",
    "    'generator_facility':  lambda x: list(x),\n",
    "    'generator_contact':  lambda x: list(x),\n",
    "    'notes': lambda x: list(x.unique()),     # Unique notes combined into a list\n",
    "    \n",
    "    # Numerical fields with aggregation functions\n",
    "    'total_reads': 'sum',\n",
    "    'total_bp': 'sum',\n",
    "    'total_Gbp': 'sum',\n",
    "    'min': 'mean',\n",
    "    'max': 'mean',\n",
    "    'quartile_25': 'mean',\n",
    "    'quartile_50': 'mean',\n",
    "    'quartile_75': 'mean',\n",
    "    'N25': 'mean',\n",
    "    'N50': 'mean',\n",
    "    'N75': 'mean'\n",
    "}\n",
    "\n",
    "# Apply aggregation with alignment maintained for lists\n",
    "aggregated_sample_df = aggregated_df.groupby('sample_ID', as_index=False).agg(aggregation_rules).reset_index()\n",
    "\n",
    "# Resulting DataFrame has lists in 'filename' and 'path' that align by index for each sample_ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_sample_df.drop(columns=['index'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_sample_df.to_csv(\"data/hprc-data-explorer-tables/HPRC_PacBio_HiFi.sample.index.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
